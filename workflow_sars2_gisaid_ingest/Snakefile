# coding: utf-8

import datetime
import gzip
import os

from pathlib import Path

configfile: "../config/config_sars2_gisaid.yaml"

envvars:
    "GISAID_URL",
    "GISAID_USERNAME",
    "GISAID_PASSWORD"

# Get today's date in ISO format (YYYY-MM-DD)
today_str = datetime.date.today().isoformat()

data_folder = os.path.join("..", config["data_folder"])
static_data_folder = os.path.join("..", config["static_data_folder"])


rule all:
    input:
        # Download latest data feed, process sequences
        download_status = os.path.join(
            data_folder, "status", "download_" + today_str + ".done"
        ),
        copy_status = os.path.join(
            data_folder, "status", "merge_sequences_" + today_str + ".done"
        ),
        # Cleaned metadata
        metadata = os.path.join(data_folder, "metadata.csv")


rule download:
    """Download the data feed JSON object from the GISAID database, using our data feed credentials."""
    output:
        feed = temp(os.path.join(data_folder, "feed.json.xz")),
        status = touch(rules.all.input.download_status)
    shell:
        "scripts/download.sh > {output.feed}"


rule process_feed:
    """Split up the data feed's individual JSON objects into metadata and fasta files. Chunk the fasta files so that every day we only reprocess the subset of fasta files that have changed. The smaller the chunk size, the more efficient the updates, but the more files on the filesystem.
    On a 48-core workstation with 128 GB RAM, aligning 200 sequences takes about 10 minutes, and this is more acceptable than having to align 1000 sequences, which takes ~1 hour. We end up with hundreds of files, but the filesystem seems to be handling it well.
    """
    input:
        feed = rules.download.output.feed,
    output:
        metadata_dirty = os.path.join(data_folder, "metadata_dirty.csv"),
        status = touch(rules.all.input.copy_status)
    params:
        fasta = directory(os.path.join(data_folder, "fasta_raw"))
    threads: workflow.cores
    shell:
        """
        python3 scripts/process_feed.py -d {input.feed} -f {params.fasta} -m {output.metadata_dirty} -p {threads}
        """


rule pangolin_lineages:
    """Assign a lineage to each sequence using pangolin
    """
    input:
        fasta = os.path.join(data_folder, "fasta_raw", "{chunk}.fa.gz")
    output:
        fasta = temp(os.path.join(data_folder, "lineages", "{chunk}.fa")),
        lineages = os.path.join(data_folder, "lineages", "{chunk}.csv")
    conda: "envs/pangolin.yaml"
    shell:
        """
        # Pangolin can only use un-gzipped fasta files
        gunzip -c {input.fasta} > {output.fasta}
        pangolin --outfile {output.lineages} {output.fasta}
        """


rule combine_lineages:
    """Combine all lineage result chunks
    """
    input:
        lineages = get_chunks
    params:
        chunk_glob = os.path.join(data_folder, "lineages", "*.csv")
    output:
        lineages = os.path.join(data_folder, "lineages.csv")
    shell:
        """
        awk '(NR == 1) || (FNR > 1)' {params.chunk_glob} > {output.lineages}
        """


rule clean_metadata:
    """Clean up metadata from GISAID
    """
    input:
        metadata_dirty = rules.process_feed.output.metadata_dirty,
        location_corrections = os.path.join(
            static_data_folder, "location_corrections.csv"
        ),
        lineages = rules.combine_lineages.output.lineages
    output:
        metadata_clean = os.path.join(data_folder, "metadata.csv")
    shell:
        """
        python3 scripts/clean_metadata.py \
            --metadata-in {input.metadata_dirty} \
            --location-corrections {input.location_corrections} \
            --lineages {input.lineages} \
            --metadata-out {output.metadata_clean}
        """
