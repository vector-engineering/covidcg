# coding: utf-8

import datetime
import os
import gzip

from pathlib import Path

configfile: "../config/config_flu_gisaid.yaml"

# Get today's date in ISO format (YYYY-MM-DD)
today_str = datetime.date.today().isoformat()

data_folder = os.path.join("..", config["data_folder"])
static_data_folder = os.path.join("..", config["static_data_folder"])

rule all:
    input:
        # Cleaned metadata
        os.path.join(data_folder, "metadata.csv"),
        os.path.join(data_folder, "metadata_virus+serotype.csv"),
        os.path.join(
            data_folder, "status", "merge_sequences_" + today_str + ".done"
        )


# Find input chunks
METADATA_CHUNKS, = glob_wildcards(os.path.join(
    data_folder, "metadata", "{metadata_chunk}.xls"
))
SEQUENCE_CHUNKS, = glob_wildcards(os.path.join(
    data_folder, "sequences", "{sequence_chunk}.dnas"
))

rule clean_metadata:
    """Clean metadata, incorporate lineage assignments into metadata
    """
    input:
        metadata_chunks = expand(
            os.path.join(data_folder, "metadata", "{metadata_chunk}.xls"),
            metadata_chunk=METADATA_CHUNKS
        )
    output:
        metadata_clean = os.path.join(data_folder, "metadata.csv"),
        metadata_virus = os.path.join(data_folder, "metadata_virus+serotype.csv")
    shell:
        """
        python3 scripts/clean_metadata.py \
            -i {input.metadata_chunks} \
            --metadata-out {output.metadata_clean} \
            --metadata-virus-out {output.metadata_virus}
        """


checkpoint chunk_sequences:
    """Split up the data feed's individual JSON objects into metadata and fasta files. 
    Chunk the fasta files so that every day we only reprocess the subset of fasta files that have changed. 
    The smaller the chunk size, the more efficient the updates, but the more files on the filesystem.
    On a 48-core workstation with 128 GB RAM, aligning 200 sequences takes about 10 minutes, 
    and this is more acceptable than having to align 1000 sequences, which takes ~1 hour. 
    We end up with hundreds of files, but the filesystem seems to be handling it well.
    """
    input:
        sequences = expand(
            os.path.join(data_folder, "sequences", "{sequence_chunk}.dnas"),
            sequence_chunk=SEQUENCE_CHUNKS
        ),
        metadata = rules.clean_metadata.output.metadata_clean
    output:
        fasta = directory(os.path.join(data_folder, "fasta_temp"))
    params:
        chunk_size = config["chunk_size"]
    shell:
        """
        python3 scripts/chunk_sequences.py \
            --sequences {input.sequences} \
            --metadata-in {input.metadata} \
            --out-fasta {output.fasta} \
            --chunk-size {params.chunk_size}
        """


def get_num_seqs(fasta_gz):
    """Get the number of entries in a gzipped fasta file
    """
    num_seqs = 0
    with gzip.open(fasta_gz, 'rt') as fp:
        for line in fp:
            # Only check the first character of each line
            if line[0] == '>':
                num_seqs += 1
    return num_seqs


def get_changed_chunks(wildcards):
    """Helper function for detecting which chunks have changed in terms of their contents 
    (measured in equality by bytes of disk space occupied). Only re-process and re-align 
    chunks which have changed. This will save us a ton of computational time, as now that there are 200K+
    isolates on GISAID, aligning them would take 1 week for the whole batch.
    """
    
    # Only run to trigger DAG re-evaluation
    checkpoint_output = checkpoints.chunk_sequences.get(**wildcards)
    chunks, = glob_wildcards(os.path.join(data_folder, "fasta_temp", "{i}.fa.gz"))

    # Keep track of which chunks have changed
    changed_chunks = []

    for chunk in chunks:
        fasta_temp_path = Path(data_folder) / "fasta_temp" / (chunk + ".fa.gz")
        fasta_raw_path = Path(data_folder) / "fasta_raw" / (chunk + ".fa.gz")

        # If the current chunk doesn't exist yet, then mark it as changed
        if (
                not fasta_raw_path.exists() or 
                not fasta_raw_path.is_file()
            ):
            changed_chunks.append(chunk)
            continue

        # Count ">" characters in both the temp and raw files
        # as a proxy for the number of sequences in each
        # If they're different, then mark as changed
        num_seqs_temp = get_num_seqs(str(fasta_temp_path))
        num_seqs_raw = get_num_seqs(str(fasta_raw_path))

        if num_seqs_temp != num_seqs_raw:
            changed_chunks.append(chunk)

    # Return a list of fasta_temp files that have changed, so that they can be copied
    # over to fasta_raw by the below `copy_changed_files` rule
    return expand(os.path.join(data_folder, "fasta_temp", "{i}.fa.gz"), i=changed_chunks)


checkpoint copy_changed_files:
    """Using the `get_changed_chunks` function, only copy fasta files which have changed
    from the purgatory `fasta_temp` folder to the `fasta_raw` folder. By copying over the files,
    it will flag to snakemake that they (and only they - not the others) will need to be
    reprocessed and realigned.
    """
    input:
        get_changed_chunks
    output:
        # Instead of explicitly defining the fasta_raw outputs
        # (and risking touching fasta files that haven't actually changed)
        # Have the output be a flag instead, that the "all" rule checks for
        # to make sure that we actually run this rule
        status = touch(os.path.join(
            data_folder, "status", "merge_sequences_" + today_str + ".done"
        ))
    params:
        data_folder = data_folder
    shell:
        """
        python3 scripts/copy_changed_files.py --files {input} --data-folder {params.data_folder}
        """
