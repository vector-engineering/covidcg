# coding: utf-8

import datetime
import os
import gzip

from pathlib import Path

import pandas as pd

configfile: "../config/config_sars2_genbank.yaml"

# Get today's date in ISO format (YYYY-MM-DD)
today_str = datetime.date.today().isoformat()

data_folder = os.path.join("..", config["data_folder"])
# static_data_folder = os.path.join("..", config["static_data_folder"])

rule all:
    input:
        # Download latest data feed, process sequences
        download_status = os.path.join(
            data_folder, "status", "download_metadata_" + today_str + ".done"
        ),
        chunk_status = os.path.join(
            data_folder, "status", "chunk_data_" + today_str + ".done"
        ),
        copy_status = os.path.join(
            data_folder, "status", "merge_sequences_" + today_str + ".done"
        ),
        # Cleaned metadata, with lineage assignments
        metadata = os.path.join(data_folder, "metadata.csv")

min_date = pd.to_datetime(config.get('min_date', '2019-12-01'))
max_date = pd.to_datetime(config.get('end_date_cutoff', datetime.date.today().isoformat()))

chunks = [d for d in pd.period_range(start=min_date, end=max_date, freq=config.get('dl_chunk_period', 'W'))]
DL_CHUNKS = [i for i in range(len(chunks))]

rule download_metadata_chunk:
    """Download the data feed in chunks, to avoid timeouts.
    """
    output:
        feed = os.path.join(data_folder, 'feed_chunks', 'feed_{chunk}.csv'),
    params:
        start_time = lambda wildcards: chunks[int(wildcards.chunk)].start_time.strftime('%Y-%m-%dT%H:%M:%S.00Z'),
        end_time = lambda wildcards: chunks[int(wildcards.chunk)].end_time.strftime('%Y-%m-%dT%H:%M:%S.00Z')
    shell:
        """
        python3 scripts/download_metadata.py \
            --start-time {params.start_time} --end-time {params.end_time} \
                > {output.feed}
        """

rule download_sequence_chunk:
    """Download the data feed metadata in chunks, to avoid timeouts.
    """
    output:
        feed = os.path.join(data_folder, 'feed_chunks', 'feed_{chunk}.fasta'),
    params:
        start_time = lambda wildcards: chunks[int(wildcards.chunk)].start_time.strftime('%Y-%m-%dT%H:%M:%S.00Z'),
        end_time = lambda wildcards: chunks[int(wildcards.chunk)].end_time.strftime('%Y-%m-%dT%H:%M:%S.00Z')
    shell:
        """
        python3 scripts/download_sequences.py \
            --start-time {params.start_time} --end-time {params.end_time} \
                > {output.feed}
        """

rule combine_metadata_chunks:
    """Combine all the downloaded chunks into a single CSV file.
    Combining CSV files without duplicating header: https://apple.stackexchange.com/a/348619
    """
    input:
        chunks = expand(os.path.join(data_folder, 'feed_chunks', 'feed_{chunk}.csv'), chunk=DL_CHUNKS)
    params:
        chunk_glob = os.path.join(data_folder, 'feed_chunks', 'feed*.csv')
    output:
        metadata = os.path.join(data_folder, 'metadata_dirty.csv'),
        status = touch(os.path.join(
            data_folder, "status", "download_metadata_" + today_str + ".done"
        ))
    shell:
        """
        awk '(NR == 1) || (FNR > 1)' {params.chunk_glob} > {output.metadata}
        """


checkpoint chunk_data:
    """Split up the data feed's individual JSON objects into metadata and fasta files. 
    Chunk the fasta files so that every day we only reprocess the subset of fasta files that 
    have changed. The smaller the chunk size, the more efficient the updates, but the more 
    files on the filesystem.
    On a 48-core workstation with 128 GB RAM, aligning 200 sequences takes about 10 minutes, 
    and this is more acceptable than having to align 1000 sequences, which takes ~1 hour. 
    We end up with hundreds of files, but the filesystem seems to be handling it well.
    """
    input:
        metadata = rules.combine_metadata_chunks.output.metadata,
        sequence_chunks = expand(os.path.join(data_folder, 'feed_chunks', 'feed_{chunk}.fasta'), chunk=DL_CHUNKS)
    output:
        status = touch(os.path.join(
            data_folder, "status", "chunk_data_" + today_str + ".done"
        ))
    params:
        fasta = os.path.join(data_folder, "fasta_temp"),
        chunk_size = config["chunk_size"]
    shell:
        """
        python3 scripts/chunk_data.py \
            --metadata {input.metadata} \
            --sequence-chunks {input.sequence_chunks} \
            --out-fasta {params.fasta} \
            --chunk-size {params.chunk_size} \
            --processes {workflow.cores}
        """


def get_num_seqs(fasta_gz):
    """Get the number of entries in a gzipped fasta file
    """
    num_seqs = 0
    with gzip.open(fasta_gz, 'rt') as fp:
        for line in fp:
            # Only check the first character of each line
            if line[0] == '>':
                num_seqs += 1
    return num_seqs


def get_changed_chunks(wildcards):
    """Helper function for detecting which chunks have changed in terms of their contents
    (measured in equality by bytes of disk space occupied). Only re-process and re-align 
    chunks which have changed. This will save us a ton of computational time, as now that 
    there are 200K+ isolates on GISAID, aligning them would take 1 week for the whole batch.
    """

    # Only run to trigger DAG re-evaluation
    #checkpoint_output = checkpoints.chunk_data.get(**wildcards)
    chunks, = glob_wildcards(os.path.join(data_folder, "fasta_temp", "{i}.fa.gz"))

    # Keep track of which chunks have changed
    changed_chunks = []

    for chunk in chunks:
        fasta_temp_path = Path(data_folder) / "fasta_temp" / (chunk + ".fa.gz")
        fasta_raw_path = Path(data_folder) / "fasta_raw" / (chunk + ".fa.gz")

        # If the current chunk doesn't exist yet, then mark it as changed
        if (
                not fasta_raw_path.exists() or
                not fasta_raw_path.is_file()
            ):
            changed_chunks.append(chunk)
            continue

        # Count ">" characters in both the temp and raw files
        # as a proxy for the number of sequences in each
        # If they're different, then mark as changed
        num_seqs_temp = get_num_seqs(str(fasta_temp_path))
        num_seqs_raw = get_num_seqs(str(fasta_raw_path))

        if num_seqs_temp != num_seqs_raw:
            changed_chunks.append(chunk)

    # Return a list of fasta_temp files that have changed, so that they can be copied
    # over to fasta_raw by the below `copy_changed_files` rule)
    return expand(os.path.join(data_folder, "fasta_temp", "{i}.fa.gz"), i=changed_chunks)


rule copy_changed_files:
    """Using the `get_changed_chunks` function, only copy fasta files which have changed
    from the purgatory `fasta_temp` folder to the `fasta_raw` folder. By copying over the files,
    it will flag to snakemake that they (and only they - not the others) will need to be
    reprocessed and realigned.
    """
    input:
        get_changed_chunks
    output:
        # Instead of explicitly defining the fasta_raw outputs
        # (and risking touching fasta files that haven't actually changed)
        # Have the output be a flag instead, that the "all" rule checks for
        # to make sure that we actually run this rule
        status = touch(os.path.join(
            data_folder, "status", "merge_sequences_" + today_str + ".done"
        ))
    params:
        data_folder = data_folder
    shell:
        """
        python3 scripts/copy_changed_files.py --files {input} --data-folder {params.data_folder}
        """


rule pangolin_lineages:
    """Assign a lineage to each sequence using pangolin
    """
    input:
        fasta = os.path.join(data_folder, "fasta_raw", "{chunk}.fa.gz")
    output:
        fasta = temp(os.path.join(data_folder, "lineages", "{chunk}.fa")),
        lineages = os.path.join(data_folder, "lineages", "{chunk}.csv")
    conda: "envs/pangolin.yaml"
    params:
        cores = int(workflow.cores / 10),
        tempdir = os.getenv("PANGOLIN_TEMPDIR", "/var/tmp")
    threads: workflow.cores / 10
    shell:
        """
        pangolin --update
        pangolin --update-data 
        # Pangolin can only use un-gzipped fasta files
        gunzip -c {input.fasta} > {output.fasta}
        pangolin --outfile {output.lineages} {output.fasta} \
            --tempdir {params.tempdir} \
            -t {params.cores} # --analysis-mode fast
        """

rule combine_lineages:
    """Combine all lineage result chunks
    """
    input:
        lineages = expand(
            os.path.join(data_folder, "lineages", "{chunk}.csv"),
            chunk=glob_wildcards(os.path.join(data_folder, "fasta_raw", "{i}.fa.gz")).i
        ),
        status = rules.all.input.copy_status
    params:
        chunk_glob = os.path.join(data_folder, "lineages", "*.csv")
    output:
        lineages = os.path.join(data_folder, "lineages.csv")
    shell:
        """
        echo {input.lineages}
        awk '(NR == 1) || (FNR > 1)' {params.chunk_glob} > {output.lineages}
        """


rule sequence_quality:
    """Collect statistics about sequence quality
    """
    input:
        fasta = os.path.join(data_folder, "fasta_raw", "{chunk}.fa.gz")
    output:
        quality = os.path.join(data_folder, "quality", "{chunk}.csv")
    shell:
        """
        python3 scripts/sequence_quality.py \
            --input {input.fasta} \
            --output {output.quality}
        """

rule combine_quality:
    """Combine all quality result chunks
    """
    input:
        quality = expand(
            os.path.join(data_folder, "quality", "{chunk}.csv"),
            chunk=glob_wildcards(os.path.join(data_folder, "fasta_raw", "{i}.fa.gz")).i
        ),
        status = rules.all.input.copy_status
    params:
        chunk_glob = os.path.join(data_folder, "quality", "*.csv")
    output:
        quality = os.path.join(data_folder, "quality.csv")
    shell:
        """
        echo {input.quality}
        awk '(NR == 1) || (FNR > 1)' {params.chunk_glob} > {output.quality}
        """


rule clean_metadata:
    """Clean metadata, incorporate lineage assignments into metadata
    """
    input:
        metadata_dirty = rules.combine_metadata_chunks.output.metadata,
        lineages = rules.combine_lineages.output.lineages,
        quality = rules.combine_quality.output.quality
    output:
        metadata_clean = os.path.join(data_folder, "metadata.csv")
    shell:
        """
        python3 scripts/clean_metadata.py \
            --metadata-in {input.metadata_dirty} \
            --lineages {input.lineages} \
            --quality {input.quality} \
            --metadata-out {output.metadata_clean}
        """
